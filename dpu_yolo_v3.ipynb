{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPU example: Yolo_v3\n",
    "\n",
    "This notebooks shows how to run a YOLO network based application for object detection. The application, as well as the DPU IP, is pulled from the official [Vitis AI Github Repository](https://github.com/Xilinx/Vitis-AI).\n",
    "For more information, please refer to the [Xilinx Vitis AI page](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).\n",
    "\n",
    "In this notebook we will be using the DNNDK **Python API** to run the DPU tasks.\n",
    "\n",
    "## 1. Prepare the overlay\n",
    "We will download the overlay onto the board. Then we will load the \n",
    "corresponding DPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq_dpu import DpuOverlay\n",
    "overlay = DpuOverlay(\"dpu.bit\")\n",
    "overlay.load_model(\"dpu_tf_yolov3.elf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants and helper functions \n",
    "\n",
    "You can view all of the helper functions in [DNNDK yolo example](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/tf_yolov3_voc.py). \n",
    "The helper functions released along with Vitis AI cover pre-processing of \n",
    "the images, so they can be normalized and resized to be compatible with \n",
    "the DPU model. These functions are included in our `pynq_dpu` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "import cv2\n",
    "import colorsys\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pynq_dpu.edge.dnndk.tf_yolov3_voc_py.tf_yolov3_voc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "Yolo V2 and V3 predict offsets from a predetermined set of boxes with \n",
    "particular height-width ratios; those predetermined set of boxes are the \n",
    "anchor boxes. We will use the predefined [anchors](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/model_data/yolo_anchors.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_list = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n",
    "anchor_float = [float(x) for x in anchor_list]\n",
    "anchors = np.array(anchor_float).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `get_class()` function in `tf_yolov3_voc` module to\n",
    "get class names from predefined [class names](https://github.com/Xilinx/Vitis-AI/blob/v1.1/mpsoc/vitis_ai_dnndk_samples/tf_yolov3_voc_py/image/voc_classes.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_path = \"img/voc_classes.txt\"\n",
    "class_names = get_class(classes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of classes, we will define a unique color for each\n",
    "class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: \n",
    "                  (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), \n",
    "                  colors))\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some DPU-related parameters, such as DPU kernel name and\n",
    "input/output node names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_CONV=\"tf_yolov3\"\n",
    "CONV_INPUT_NODE=\"conv2d_1_convolution\"\n",
    "CONV_OUTPUT_NODE1=\"conv2d_59_convolution\"\n",
    "CONV_OUTPUT_NODE2=\"conv2d_67_convolution\"\n",
    "CONV_OUTPUT_NODE3=\"conv2d_75_convolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing bounding boxes\n",
    "We now define a custom function that draws the bounding boxes around \n",
    "the identified objects after we have the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, scores, classes):\n",
    "    _, ax = plt.subplots(1)\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        [top, left, bottom, right] = bbox\n",
    "        width, height = right - left, bottom - top\n",
    "        center_x, center_y = left + width*0.5, top + height*0.5\n",
    "        score, class_index = scores[i], classes[i]\n",
    "        label = '{}: {:.4f}'.format(class_names[class_index], score) \n",
    "        color = tuple([color/255 for color in colors[class_index]])\n",
    "        ax.add_patch(Rectangle((left, top), width, height,\n",
    "                               edgecolor=color, facecolor='none'))\n",
    "        ax.annotate(label, (center_x, center_y), color=color, weight='bold', \n",
    "                    fontsize=12, ha='center', va='center')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting classes\n",
    "We need to define a function that evaluates the scores and makes predictions\n",
    "based on the provided class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(yolo_outputs, image_shape, class_names, anchors):\n",
    "    score_thresh = 0.2\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    input_shape = np.shape(yolo_outputs[0])[1 : 3]\n",
    "    input_shape = np.array(input_shape)*32\n",
    "\n",
    "    for i in range(len(yolo_outputs)):\n",
    "        _boxes, _box_scores = boxes_and_scores(\n",
    "            yolo_outputs[i], anchors[anchor_mask[i]], len(class_names), \n",
    "            input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = np.concatenate(boxes, axis = 0)\n",
    "    box_scores = np.concatenate(box_scores, axis = 0)\n",
    "\n",
    "    mask = box_scores >= score_thresh\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(len(class_names)):\n",
    "        class_boxes_np = boxes[mask[:, c]]\n",
    "        class_box_scores_np = box_scores[:, c]\n",
    "        class_box_scores_np = class_box_scores_np[mask[:, c]]\n",
    "        nms_index_np = nms_boxes(class_boxes_np, class_box_scores_np) \n",
    "        class_boxes_np = class_boxes_np[nms_index_np]\n",
    "        class_box_scores_np = class_box_scores_np[nms_index_np]\n",
    "        classes_np = np.ones_like(class_box_scores_np, dtype = np.int32) * c\n",
    "        boxes_.append(class_boxes_np)\n",
    "        scores_.append(class_box_scores_np)\n",
    "        classes_.append(classes_np)\n",
    "    boxes_ = np.concatenate(boxes_, axis = 0)\n",
    "    scores_ = np.concatenate(scores_, axis = 0)\n",
    "    classes_ = np.concatenate(classes_, axis = 0)\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original picture\n",
    "Let's also have a look at the original picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"img/jinrikisha-911722.JPEG\"\n",
    "image = cv2.imread(image_path)\n",
    "_, ax = plt.subplots(1)\n",
    "_ = ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will normalize the input picture size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = image.shape[:2]\n",
    "image_data = np.array(pre_process(image, (416, 416)), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run application\n",
    "\n",
    "We create DPU kernel and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2cube.dpuOpen()\n",
    "kernel = n2cube.dpuLoadKernel(KERNEL_CONV)\n",
    "task = n2cube.dpuCreateTask(kernel, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the DPU task to classify an input picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "input_len = n2cube.dpuGetInputTensorSize(task, CONV_INPUT_NODE)\n",
    "n2cube.dpuSetInputTensorInHWCFP32(\n",
    "    task, CONV_INPUT_NODE, image_data, input_len)\n",
    "\n",
    "n2cube.dpuRunTask(task)\n",
    "\n",
    "conv_sbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE1)\n",
    "conv_out1 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE1, \n",
    "                                               conv_sbbox_size)\n",
    "conv_out1 = np.reshape(conv_out1, (1, 13, 13, 75))\n",
    "\n",
    "conv_mbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE2)\n",
    "conv_out2 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE2, \n",
    "                                               conv_mbbox_size)\n",
    "conv_out2 = np.reshape(conv_out2, (1, 26, 26, 75))\n",
    "\n",
    "conv_lbbox_size = n2cube.dpuGetOutputTensorSize(task, CONV_OUTPUT_NODE3)\n",
    "conv_out3 = n2cube.dpuGetOutputTensorInHWCFP32(task, CONV_OUTPUT_NODE3, \n",
    "                                               conv_lbbox_size)\n",
    "conv_out3 = np.reshape(conv_out3, (1, 52, 52, 75))\n",
    "\n",
    "yolo_outputs = [conv_out1, conv_out2, conv_out3]   \n",
    "\n",
    "end_time = time()\n",
    "print(\"time {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `evaluate()` function defined previously now. Depending on\n",
    "how many objects can be identified in the picture, the function returns\n",
    "a list of bounding box coordinates, a list of probability scores, and a list\n",
    "of class indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, scores, classes = evaluate(yolo_outputs, image_size, \n",
    "                                  class_names, anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the bounding boxes, classes, and scores, we can feed them to\n",
    "the `draw_boxes()` function to draw the bounding boxes over each recognized object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = draw_boxes(image, boxes, scores, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell probably only shows 1 class. Keep in mind that the model\n",
    "is able to identify multiple projects in the given picture.\n",
    "You can try a different picture by specifying a new `image_path`. \n",
    "\n",
    "Remember to clean up when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2cube.dpuDestroyTask(task)\n",
    "n2cube.dpuDestroyKernel(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2020 Xilinx, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
